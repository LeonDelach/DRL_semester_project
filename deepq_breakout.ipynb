{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536027fb",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fdc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b32ef",
   "metadata": {},
   "source": [
    "## Environment and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0520a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\")  # render_mode=\"human\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf7c59",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88343258",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3639b4f",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d96d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim_x, dim_y, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        self.fc1 = torch.nn.Linear(18 * dim_x * dim_y // 4, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, n_actions)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)  # start_dim=1 because we want to process batches as well as single elements\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169189b",
   "metadata": {},
   "source": [
    "## Parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04facbbb",
   "metadata": {},
   "source": [
    "## Variables initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea28b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "obs_x = state.shape[0]\n",
    "obs_y = state.shape[1]\n",
    "\n",
    "policy_net = DQN(obs_x, obs_y, n_actions).to(device)\n",
    "target_net = DQN(obs_x, obs_y, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f38e6",
   "metadata": {},
   "source": [
    "## Select action with epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157f8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon():\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    return eps_threshold\n",
    "\n",
    "def select_action(state, eps_threshold):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    steps_done += 1\n",
    "    # epsilon-greedy\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(torch.moveaxis(state, -1, 1))).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0eac4a",
   "metadata": {},
   "source": [
    "## One step optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0f6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))   # list of transitions to transition with lists as state, reward, ...\n",
    "\n",
    "    # using batch.next_state\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(torch.moveaxis(state_batch, -1, 1)).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # using target_net to improve stability because V(s') = max Q(s', a') that is used in the update rule\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(torch.moveaxis(non_final_next_states, -1, 1)).max(1)[0]\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0931ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee41af9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7af60e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "#\n",
      "140\n",
      "201\n",
      "216\n",
      "#\n",
      "245\n",
      "185\n",
      "125\n",
      "#\n",
      "131\n",
      "172\n",
      "136\n",
      "#\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state, update_epsilon())\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        # env.render()\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            print(t)\n",
    "            break\n",
    "            \n",
    "    if (i_episode % 3) == 0:  \n",
    "        print(\"#\")\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()    \n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f93c90",
   "metadata": {},
   "source": [
    "## Test one game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90eeb9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALE/Breakout-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, 0)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    env.render()\n",
    "\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
